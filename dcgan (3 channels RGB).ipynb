{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.15.0 in c:\\users\\public\\efendim\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\public\\efendim\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\public\\efendim\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\public\\efendim\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\public\\efendim\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\public\\efendim\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\public\\efendim\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\public\\efendim\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\public\\efendim\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\public\\efendim\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\public\\efendim\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\public\\efendim\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\public\\efendim\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\public\\efendim\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow==2.15.0) (3.2.2)\n",
      "Requirement already satisfied: keras==2.15.0 in c:\\users\\public\\efendim\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: imageio in c:\\users\\public\\efendim\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\public\\efendim\\lib\\site-packages (from imageio) (1.24.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\public\\efendim\\lib\\site-packages (from imageio) (9.4.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\public\\efendim\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\public\\efendim\\lib\\site-packages (from opencv-python) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.15.0\n",
    "!pip install keras==2.15.0\n",
    "\n",
    "\n",
    "!pip install imageio\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbb3a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Public\\Efendim\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import neccessary libraries\n",
    "from keras.layers import UpSampling2D, Reshape, Activation, Conv2D, BatchNormalization, LeakyReLU, Input, Flatten, multiply, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import cv2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.optimizers import Adam as LegacyAdam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten, Input\n",
    "from keras.optimizers import Adam as LegacyAdam\n",
    "import tensorflow as tf  \n",
    "\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0da3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d068482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\net pc\\AppData\\Local\\Temp\\ipykernel_20768\\2614605422.py:14: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize(target_size, Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glioma_tumor image shape: (100, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Function to load images from a directory\n",
    "\n",
    "# Define the paths\n",
    "glioma_tumor_path = \"C:\\\\Users\\\\net pc\\\\Desktop\\\\Thesis Paper 3\\\\Brain tumor dataset\\\\Testing\\\\glioma_tumor\"\n",
    "\n",
    "\n",
    "\n",
    "def load_and_resize_images(folder_path, target_size=(64, 64)):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img = img.resize(target_size, Image.ANTIALIAS)\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "    return np.array(images)\n",
    "\n",
    "# Assuming glioma_tumor is the path as previously defined\n",
    "glioma_tumor_images = load_and_resize_images(glioma_tumor_path)\n",
    "\n",
    "\n",
    "\n",
    "print(\"glioma_tumor image shape:\", glioma_tumor_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07314ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glioma_tumor_images = glioma_tumor_images.reshape(glioma_tumor_images.shape[0], 64, 64, 3).astype('float32')\n",
    "glioma_tumor_images = (glioma_tumor_images - 127.5) / 127.5  # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb294f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Choose your desired batch size\n",
    "glioma_tumor_images = tf.data.Dataset.from_tensor_slices(glioma_tumor_images).batch(batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ebe798",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_BatchDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display the image\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(glioma_tumor_images[index])\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mTypeError\u001b[0m: '_BatchDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming glioma_tumor_images is the array containing the loaded images\n",
    "# Choose an index to view a specific image\n",
    "index = 0\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(glioma_tumor_images[index])\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ec340",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" image size:\", glioma_tumor_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mode=None\n",
    "batch_size=32\n",
    "shuffle=True\n",
    "seed=None\n",
    "validation_split=None\n",
    "(lambda x: x / 255.0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64, 64, 3)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "print(discriminator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf41776",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = keras.optimizers.Adam(1e-4)\n",
    "opt_disc = keras.optimizers.Adam(1e-4)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de3465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c8150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ff5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2):\n",
    "    for idx, real in enumerate(tqdm(glioma_tumor_images)):\n",
    "        batch_size = real.shape[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "        fake = generator(random_latent_vectors)\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            fig = plt.figure(figsize=(16, 16))  # Adjust figure size\n",
    "            for i in range(1):  # Change to 16 if you want 4x4 grid\n",
    "                plt.subplot(4, 4, i+1)  # Adjust subplot parameters for 4x4 grid\n",
    "                plt.imshow(fake[i, :, :, 0] * 127.5 + 127.5, cmap='gray')  # Adjust colormap and scaling\n",
    "                plt.axis('off')\n",
    "            plt.savefig(f\"generated_images/generated_img{epoch}_{idx}.png\", bbox_inches='tight')  # Save figure with tight bounding box\n",
    "            plt.close(fig)  # Close the figure to release memory\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z))\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            loss_disc_real = loss_fn(tf.ones((batch_size, 1)), discriminator(real))\n",
    "            loss_disc_fake = loss_fn(tf.zeros((batch_size, 1)), discriminator(fake))\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "        grads = disc_tape.gradient(loss_disc, discriminator.trainable_weights)\n",
    "        opt_disc.apply_gradients(\n",
    "            zip(grads, discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        ### Train Generator min log(1 - D(G(z)) <-> max log(D(G(z))\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake = generator(random_latent_vectors)\n",
    "            output = discriminator(fake)\n",
    "            loss_gen = loss_fn(tf.ones((batch_size, 1)), output)\n",
    "\n",
    "        grads = gen_tape.gradient(loss_gen, generator.trainable_weights)\n",
    "        opt_gen.apply_gradients(\n",
    "            zip(grads, generator.trainable_weights)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e606c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c52b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67434a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d7b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ea32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
